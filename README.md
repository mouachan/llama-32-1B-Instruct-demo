# üöÄ D√©ploiement du mod√®le Llama-3.2-1B-Instruct sur OpenShift AI

Ce projet permet de d√©ployer le mod√®le `meta-llama/Llama-3.2-1B-Instruct` sur OpenShift AI en utilisant vLLM comme runtime d'inf√©rence.

## üìã Pr√©requis

- **OpenShift CLI** (`oc`) install√© et configur√©
- **Acc√®s √† un cluster OpenShift** avec OpenShift AI install√©
- **Token Hugging Face** avec acc√®s au mod√®le `meta-llama/Llama-3.2-1B-Instruct`
- **GPU disponible** sur le cluster (minimum 1 GPU avec 8GB VRAM)
- **Python 3.8+** et **pip** pour les tests

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    OpenShift                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Secret: huggingface-token                                ‚îÇ
‚îÇ  PVC: pvc-llama32-1b-instruct                            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Mod√®le t√©l√©charg√© depuis Hugging Face                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  OpenShift AI                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  DataConnection: llama-model-pvc-connection                ‚îÇ
‚îÇ  ServingRuntime: llama-32-1b-instruct (Serving Model)     ‚îÇ
‚îÇ  InferenceService: llama-32-1b-instruct                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Pod avec GPU                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ D√©ploiement √©tape par √©tape

### 1. Configuration de l'environnement

```bash
# Copier le template d'environnement
cp env-template .env

# √âditer le fichier .env avec vos valeurs
nano .env
```

**Variables obligatoires √† configurer :**
- `OPENSHIFT_CLUSTER_URL` : URL de l'API OpenShift
- `OPENSHIFT_CLUSTER_DOMAIN` : Domaine de votre cluster
- `HUGGINGFACE_TOKEN` : Votre token Hugging Face
- `GPU_TYPE` : Type de GPU disponible sur votre cluster

### 2. Connexion √† OpenShift

```bash
# Se connecter √† votre cluster
oc login $OPENSHIFT_CLUSTER_URL

# V√©rifier la connexion
oc whoami
```

### 3. D√©ploiement manuel √©tape par √©tape

Si vous pr√©f√©rez d√©ployer manuellement, voici les commandes √† ex√©cuter dans l'ordre :

#### 3.1 Cr√©ation du namespace

```bash
# Cr√©er le namespace avec les labels OpenShift AI
oc apply -f namespace.yaml
```

#### 3.2 Cr√©ation du secret Hugging Face

```bash
# Cr√©er le secret avec votre token HF
oc apply -f secret-huggingface.yaml
```

#### 3.3 Cr√©ation du PVC

```bash
# Cr√©er le PVC pour stocker le mod√®le
oc apply -f pvc-llama32-1b-instruct.yaml
```

#### 3.4 Cr√©ation de la connexion PVC

```bash
# Cr√©er la connexion PVC pour OpenShift AI
oc apply -f secret-llama-model-pvc-connection.yaml
```

#### 3.5 T√©l√©chargement du mod√®le

```bash
# Lancer le job de t√©l√©chargement
oc apply -f job-download-hf-cli.yaml

# Attendre la fin du t√©l√©chargement
oc wait --for=condition=Complete job/download-llama32-1b-instruct-hf-cli -n llama-instruct-32-1b-demo --timeout=1800s

# V√©rifier le contenu du PVC
oc get pvc pvc-llama32-1b-instruct -n llama-instruct-32-1b-demo
```

#### 3.6 V√©rification du contenu du PVC

```bash
# Attendre que le PVC soit stable
sleep 10

# Cr√©er un pod temporaire pour v√©rifier le contenu
oc apply -f pod-check-pvc.yaml

# Attendre que le pod de v√©rification se termine
oc wait --for=condition=Complete pod/check-pvc-content -n llama-instruct-32-1b-demo --timeout=60s

# Afficher le contenu
oc logs check-pvc-content -n llama-instruct-32-1b-demo

# Nettoyer le pod de v√©rification
oc delete pod check-pvc-content -n llama-instruct-32-1b-demo --ignore-not-found=true

# V√©rifier que le mod√®le principal est pr√©sent
if oc logs check-pvc-content -n llama-instruct-32-1b-demo 2>/dev/null | grep -q "model.safetensors"; then
    echo "‚úÖ Mod√®le t√©l√©charg√© avec succ√®s !"
else
    echo "‚ùå Mod√®le non trouv√© dans le PVC !"
    echo "V√©rifiez les logs du job de t√©l√©chargement :"
    oc logs job/download-llama32-1b-instruct-hf-cli -n llama-instruct-32-1b-demo
    exit 1
fi
```

#### 3.7 Cr√©ation du ServingRuntime

```bash
# Cr√©er le runtime vLLM
oc apply -f servingruntime-llama32-1b.yaml
```

#### 3.8 Cr√©ation de l'InferenceService

```bash
# Cr√©er le service d'inf√©rence
oc apply -f inferenceservice-llama32-1b.yaml

# Attendre que le service soit pr√™t
oc wait --for=condition=Ready inferenceservice/llama-32-1b-instruct -n llama-instruct-32-1b-demo --timeout=600s
```

### 4. D√©ploiement automatique

Pour un d√©ploiement automatique et plus simple :

```bash
# Rendre le script ex√©cutable
chmod +x deploy.sh

# Lancer le d√©ploiement complet
./deploy.sh
```

**Ce que fait le script :**
1. ‚úÖ Cr√©ation du namespace `llama-instruct-32-1b-demo`
2. ‚úÖ Cr√©ation du secret avec le token Hugging Face
3. ‚úÖ Cr√©ation du PVC pour stocker le mod√®le
4. ‚úÖ Cr√©ation du secret de connexion PVC
5. ‚úÖ T√©l√©chargement du mod√®le depuis Hugging Face
6. ‚úÖ V√©rification du contenu du PVC
7. ‚úÖ Cr√©ation du ServingRuntime (Serving Model)
8. ‚úÖ Cr√©ation de l'InferenceService

### 5. V√©rification du d√©ploiement

```bash
# V√©rifier le statut de l'InferenceService
oc get inferenceservice -n llama-instruct-32-1b-demo

# V√©rifier les pods
oc get pods -n llama-instruct-32-1b-demo

# V√©rifier le PVC
oc get pvc -n llama-instruct-32-1b-demo

# V√©rifier les secrets
oc get secrets -n llama-instruct-32-1b-demo

# V√©rifier les √©v√©nements
oc get events -n llama-instruct-32-1b-demo --sort-by='.lastTimestamp'
```

### 6. Test du mod√®le

```bash
# Test avec Python
python3 test-llama-model.py

# Test avec curl
./test-llama-curl.sh

# Test manuel avec curl
curl -X POST "https://llama-32-1b-instruct-llama-instruct-32-1b-demo.apps.VOTRE_CLUSTER/v1/models" \
  -H "Content-Type: application/json"
```

## üóëÔ∏è Nettoyage et gestion

### Utilisation du script cleanup.sh

Le script `cleanup.sh` permet de supprimer proprement toutes les ressources cr√©√©es :

```bash
# Rendre le script ex√©cutable
chmod +x cleanup.sh

# Lancer le nettoyage
./cleanup.sh
```

**Ce que fait le script :**
1. üîç V√©rification de la connexion OpenShift
2. ‚ö†Ô∏è Demande de confirmation avant suppression
3. üö´ Suppression de l'InferenceService
4. ‚öôÔ∏è Suppression du ServingRuntime
5. üì• Suppression des jobs de t√©l√©chargement
6. üîÑ Suppression des pods restants
7. üîê Suppression des secrets
8. üíæ Suppression du PVC
9. üìÅ Suppression du namespace complet

**‚ö†Ô∏è Attention :** Ce script supprime TOUTES les ressources du projet !

### Nettoyage manuel

Si vous pr√©f√©rez nettoyer manuellement :

```bash
# Supprimer l'InferenceService
oc delete inferenceservice llama-32-1b-instruct -n llama-instruct-32-1b-demo

# Supprimer le ServingRuntime
oc delete servingruntime llama-32-1b-instruct -n llama-instruct-32-1b-demo

# Supprimer les jobs
oc delete job download-llama32-1b-instruct-hf-cli -n llama-instruct-32-1b-demo

# Supprimer les pods
oc delete pods --all -n llama-instruct-32-1b-demo

# Supprimer les secrets
oc delete secret huggingface-token -n llama-instruct-32-1b-demo
oc delete secret llama-model-pvc-connection -n llama-instruct-32-1b-demo

# Supprimer le PVC
oc delete pvc pvc-llama32-1b-instruct -n llama-instruct-32-1b-demo

# Supprimer le namespace (supprime tout le reste)
oc delete namespace llama-instruct-32-1b-demo
```

### Red√©ploiement

Pour red√©ployer apr√®s un nettoyage :

```bash
# Nettoyer d'abord
./cleanup.sh

# Red√©ployer
./deploy.sh
```

## üîß Configuration d√©taill√©e

### Variables d'environnement

| Variable | Description | Exemple |
|----------|-------------|---------|
| `OPENSHIFT_CLUSTER_URL` | URL de l'API OpenShift | `https://api.cluster.example.com:6443` |
| `OPENSHIFT_PROJECT` | Nom du projet/namespace | `llama-instruct-32-1b-demo` |
| `OPENSHIFT_CLUSTER_DOMAIN` | Domaine du cluster | `cluster.example.com` |
| `HUGGINGFACE_TOKEN` | Token d'acc√®s Hugging Face | `hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx` |
| `HUGGINGFACE_MODEL` | Mod√®le √† t√©l√©charger | `meta-llama/Llama-3.2-1B-Instruct` |
| `GPU_TYPE` | Type de GPU | `NVIDIA-A10G-PRIVATE` |
| `GPU_COUNT` | Nombre de GPUs | `1` |
| `GPU_MEMORY_UTILIZATION` | Utilisation m√©moire GPU | `0.85` |
| `MODEL_MAX_LENGTH` | Longueur maximale du mod√®le | `4096` |
| `CPU_REQUEST` | CPU demand√© | `2` |
| `CPU_LIMIT` | CPU maximum | `4` |
| `MEMORY_REQUEST` | M√©moire demand√©e | `6Gi` |
| `MEMORY_LIMIT` | M√©moire maximum | `8Gi` |
| `PVC_SIZE` | Taille du PVC | `10Gi` |
| `STORAGE_CLASS` | Classe de stockage | `gp3-csi` |
| `API_PORT` | Port de l'API | `8080` |

### Ressources cr√©√©es

- **Namespace** : `llama-instruct-32-1b-demo`
- **PVC** : `pvc-llama32-1b-instruct` (10Gi)
- **Secret** : `huggingface-token` (token HF)
- **Secret** : `llama-model-pvc-connection` (connexion PVC)
- **ServingRuntime** : `llama-32-1b-instruct` (vLLM)
- **InferenceService** : `llama-32-1b-instruct`
- **Job** : `download-llama32-1b-instruct-hf-cli`

## üß™ Tests

### Test des endpoints

Le mod√®le expose les endpoints suivants :

### Test de la fonctionnalit√© RAG (optionnel)

Si vous souhaitez utiliser la fonctionnalit√© RAG avec Docling :

```bash
# Aller dans le r√©pertoire RAG
cd llamastack/rag

# D√©ployer la fonctionnalit√© RAG
./deploy-rag.sh

# Ou d√©ployer manuellement
oc apply -f docling-pipeline.yaml

# Tester la fonctionnalit√© RAG
python3 test-rag.py
```

Pour plus de d√©tails, consultez le [README RAG](llamastack/rag/README.md).

- **`/v1/models`** : Liste des mod√®les disponibles
- **`/v1/chat/completions`** : Chat conversationnel
- **`/v1/completions`** : G√©n√©ration de texte

### Exemple de requ√™te chat

```bash
curl -X POST "https://llama-32-1b-instruct-llama-instruct-32-1b-demo.apps.VOTRE_CLUSTER/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-32-1b-instruct",
    "messages": [
      {"role": "user", "content": "Bonjour, comment allez-vous ?"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## üîç D√©pannage

### Probl√®mes courants

1. **Token Hugging Face invalide**
   - V√©rifiez que votre token a acc√®s au mod√®le
   - Testez l'acc√®s : `curl -H "Authorization: Bearer $TOKEN" https://huggingface.co/api/models/meta-llama/Llama-3.2-1B-Instruct`

2. **GPU non disponible**
   - V√©rifiez les ressources GPU : `oc get nodes -o json | jq '.items[].status.allocatable'`
   - Ajustez `GPU_TYPE` dans `.env`

3. **PVC non bound**
   - V√©rifiez la classe de stockage : `oc get storageclass`
   - Ajustez `STORAGE_CLASS` dans `.env`

4. **Mod√®le non accessible**
   - V√©rifiez les logs du pod : `oc logs -f llama-32-1b-instruct-predictor-xxx`
   - V√©rifiez que le mod√®le est t√©l√©charg√© dans le PVC

### Logs utiles

```bash
# Logs de l'InferenceService
oc logs -f deployment/llama-32-1b-instruct-predictor

# Logs du job de t√©l√©chargement
oc logs -f job/download-llama32-1b-instruct-hf-cli

# √âv√©nements du namespace
oc get events -n llama-instruct-32-1b-demo
```

## üìö Ressources

- [Documentation Red Hat OpenShift AI Self-Managed](https://docs.redhat.com/es/documentation/red_hat_openshift_ai_self-managed/2.22)
- [Serving Models - Red Hat OpenShift AI](https://docs.redhat.com/es/documentation/red_hat_openshift_ai_self-managed/2.22/html-single/serving_models/index)
- [Documentation Hugging Face](https://huggingface.co/docs)
- [Mod√®le Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)

## üìÅ Structure des fichiers

```
llama-3.2-1B-Instruct-demo/
‚îú‚îÄ‚îÄ .env                           # Variables d'environnement (√† cr√©er)
‚îú‚îÄ‚îÄ env-template                   # Template des variables
‚îú‚îÄ‚îÄ .gitignore                     # Fichiers √† ignorer par Git
‚îú‚îÄ‚îÄ README.md                      # Cette documentation
‚îú‚îÄ‚îÄ deploy.sh                      # Script de d√©ploiement
‚îú‚îÄ‚îÄ cleanup.sh                     # Script de nettoyage
‚îú‚îÄ‚îÄ namespace.yaml                 # Configuration du namespace
‚îú‚îÄ‚îÄ secret-huggingface.yaml        # Secret Hugging Face
‚îú‚îÄ‚îÄ pvc-llama32-1b-instruct.yaml  # PVC pour le mod√®le
‚îú‚îÄ‚îÄ secret-llama-model-pvc-connection.yaml  # Connexion PVC
‚îú‚îÄ‚îÄ job-download-hf-cli.yaml       # Job de t√©l√©chargement
‚îú‚îÄ‚îÄ servingruntime-llama32-1b.yaml # Runtime vLLM
‚îú‚îÄ‚îÄ inferenceservice-llama32-1b.yaml # Service d'inf√©rence
‚îú‚îÄ‚îÄ test-llama-model.py            # Tests Python
‚îú‚îÄ‚îÄ test-llama-curl.sh             # Tests curl
‚îî‚îÄ‚îÄ llamastack/                    # Configuration LlamaStack
    ‚îú‚îÄ‚îÄ llama-stack-inference-model-secret.yaml  # Secret pour LlamaStack
    ‚îú‚îÄ‚îÄ llama-stack-distribution.yaml            # Distribution LlamaStack
    ‚îî‚îÄ‚îÄ rag/                       # Configuration RAG avec Docling
        ‚îú‚îÄ‚îÄ README.md              # Documentation RAG
        ‚îú‚îÄ‚îÄ docling-pipeline.yaml  # Pipeline Tekton pour l'ingestion
        ‚îú‚îÄ‚îÄ deploy-rag.sh         # Script de d√©ploiement RAG
        ‚îî‚îÄ‚îÄ test-rag.py           # Script de test RAG
```

## ü§ù Contribution

Pour contribuer √† ce projet :

1. Fork le repository
2. Cr√©ez une branche pour votre fonctionnalit√©
3. Committez vos changements
4. Poussez vers la branche
5. Ouvrez une Pull Request

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de d√©tails.

---

**üéâ Bon d√©ploiement !**
