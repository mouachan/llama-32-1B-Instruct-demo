{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Test LlamaStack dans OpenShift AI\n",
    "\n",
    "Ce notebook teste la fonctionnalit√© LlamaStack avec le mod√®le Llama-3.2-1B-Instruct d√©ploy√© sur OpenShift AI.\n",
    "\n",
    "## üìã Pr√©requis\n",
    "- Namespace: `llama-instruct-32-1b-demo`\n",
    "- LlamaStackDistribution: `lsd-llama-32-1b-instruct`\n",
    "- InferenceService: `llama-32-1b-instruct`\n",
    "\n",
    "## üîó Services utilis√©s\n",
    "- `lsd-llama-32-1b-instruct-service:8321` (LlamaStack)\n",
    "- `llama-32-1b-instruct-predictor:80` (vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install llama-stack-client fire requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies\n",
    "import requests\n",
    "import json\n",
    "from llama_stack_client import Client, Agent, AgentEventLogger\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Librairies import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Test 1: V√©rification des services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de connexion √† LlamaStack\n",
    "llamastack_url = \"http://lsd-llama-32-1b-instruct-service:8321\"\n",
    "\n",
    "print(f\"üîó Test de connexion √† LlamaStack: {llamastack_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{llamastack_url}/v1/models\", timeout=10)\n",
    "    print(f\"‚úÖ Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"üìã Mod√®les disponibles:\")\n",
    "        for model in models.get('data', []):\n",
    "            print(f\"  - {model.get('identifier', 'N/A')} ({model.get('model_type', 'N/A')})\")\n",
    "    else:\n",
    "        print(f\"‚ùå Erreur: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de connexion √† vLLM\n",
    "vllm_url = \"http://llama-32-1b-instruct-predictor:80\"\n",
    "\n",
    "print(f\"üîó Test de connexion √† vLLM: {vllm_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{vllm_url}/v1/models\", timeout=10)\n",
    "    print(f\"‚úÖ Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"üìã Mod√®les vLLM disponibles:\")\n",
    "        for model in models.get('data', []):\n",
    "            print(f\"  - {model.get('id', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Erreur: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Test 2: Client LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion au client LlamaStack\n",
    "try:\n",
    "    client = Client(base_url=llamastack_url)\n",
    "    print(\"‚úÖ Client LlamaStack connect√©\")\n",
    "    \n",
    "    # Lister les mod√®les\n",
    "    models = client.models.list()\n",
    "    print(f\"üìã {len(models)} mod√®les trouv√©s:\")\n",
    "    for model in models:\n",
    "        print(f\"  - {model.identifier} ({model.model_type})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur client LlamaStack: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier les bases vectorielles\n",
    "try:\n",
    "    vector_dbs = client.vector_dbs.list()\n",
    "    print(f\"üóÑÔ∏è {len(vector_dbs)} bases vectorielles trouv√©es:\")\n",
    "    for db in vector_dbs:\n",
    "        print(f\"  - {db.identifier}\")\n",
    "        \n",
    "    if not vector_dbs:\n",
    "        print(\"‚ÑπÔ∏è Aucune base vectorielle trouv√©e (normal si pas encore d'ingestion)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur bases vectorielles: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Test 3: Agent LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un agent LlamaStack\n",
    "try:\n",
    "    # Trouver le mod√®le LLM\n",
    "    llm_model = None\n",
    "    for model in models:\n",
    "        if model.model_type == \"llm\":\n",
    "            llm_model = model\n",
    "            break\n",
    "    \n",
    "    if llm_model:\n",
    "        print(f\"ü§ñ Mod√®le LLM trouv√©: {llm_model.identifier}\")\n",
    "        \n",
    "        # Cr√©er un agent simple (sans RAG pour l'instant)\n",
    "        agent = Agent(\n",
    "            client,\n",
    "            model=llm_model.identifier,\n",
    "            instructions=\"Tu es un assistant IA utile et amical.\"\n",
    "        )\n",
    "        print(\"‚úÖ Agent LlamaStack cr√©√©\")\n",
    "        \n",
    "        # Test simple\n",
    "        session_id = agent.create_session(session_name=f\"test_session_{uuid.uuid4().hex[:8]}\")\n",
    "        print(f\"üìù Session cr√©√©e: {session_id}\")\n",
    "        \n",
    "        # Question simple\n",
    "        response = agent.create_turn(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Bonjour, comment allez-vous ?\"}],\n",
    "            session_id=session_id,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        print(\"üí¨ R√©ponse:\")\n",
    "        for log in AgentEventLogger().log(response):\n",
    "            if hasattr(log, 'content') and log.content:\n",
    "                print(log.content, end='', flush=True)\n",
    "        print()\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Aucun mod√®le LLM trouv√©\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Test 4: Test direct vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test direct de l'API vLLM\n",
    "print(\"üîß Test direct de l'API vLLM\")\n",
    "\n",
    "try:\n",
    "    # Test de chat completion\n",
    "    chat_payload = {\n",
    "        \"model\": \"llama-32-1b-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explique-moi ce qu'est l'intelligence artificielle en 2 phrases.\"}\n",
    "        ],\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{vllm_url}/v1/chat/completions\",\n",
    "        json=chat_payload,\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"‚úÖ R√©ponse vLLM:\")\n",
    "        print(result['choices'][0]['message']['content'])\n",
    "    else:\n",
    "        print(f\"‚ùå Erreur vLLM: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur test vLLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä R√©sum√© des tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Tests termin√©s !\")\n",
    "print(\"\\nüìã Prochaines √©tapes:\")\n",
    "print(\"1. Si les tests passent, d√©ployer la pipeline d'ingestion RAG\")\n",
    "print(\"2. Tester la fonctionnalit√© RAG avec des documents\")\n",
    "print(\"3. Utiliser le use case assurance\")\n",
    "print(\"\\nüîó Services test√©s:\")\n",
    "print(f\"  - LlamaStack: {llamastack_url}\")\n",
    "print(f\"  - vLLM: {vllm_url}\")\n",
    "print(\"\\nüìö Documentation:\")\n",
    "print(\"  - README RAG: llamastack/rag/README.md\")\n",
    "print(\"  - Scripts de d√©ploiement: deploy-rag.sh, deploy-assurance.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜï Test 5: Enregistrement de la base vectorielle (selon doc Red Hat)\n",
    "\n",
    "Test de l'approche recommand√©e par la documentation Red Hat pour enregistrer une base vectorielle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Enregistrement de la base vectorielle selon la doc Red Hat\n",
    "print(\"üîß Test 5: Enregistrement de la base vectorielle\")\n",
    "\n",
    "# Configuration selon la documentation Red Hat\n",
    "vector_db_id = \"assurance_minio_db\"\n",
    "embedding_model_id = \"granite-embedding-125m\"\n",
    "embedding_dimension = 125\n",
    "provider_id = \"milvus\"\n",
    "\n",
    "try:\n",
    "    # Enregistrer la base vectorielle avec l'approche recommand√©e\n",
    "    print(f\"üìù Enregistrement de la base vectorielle: {vector_db_id}\")\n",
    "    \n",
    "    result = client.vector_dbs.register(\n",
    "        vector_db_id=vector_db_id,\n",
    "        embedding_model=embedding_model_id,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        provider_id=provider_id,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Base vectorielle enregistr√©e: {result}\")\n",
    "    \n",
    "    # V√©rifier que la base est bien enregistr√©e\n",
    "    vector_dbs = client.vector_dbs.list()\n",
    "    print(f\"üóÑÔ∏è Bases vectorielles disponibles: {len(vector_dbs)}\")\n",
    "    for db in vector_dbs:\n",
    "        print(f\"  - {db.get('id', 'N/A')}: {db.get('name', 'N/A')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'enregistrement: {e}\")\n",
    "    print(f\"Type d'erreur: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜï Test 6: Agent RAG avec outil knowledge_search\n",
    "\n",
    "Test de l'agent RAG avec l'outil `builtin::rag/knowledge_search` selon la documentation Red Hat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Agent RAG avec outil knowledge_search\n",
    "print(\"ü§ñ Test 6: Agent RAG avec outil knowledge_search\")\n",
    "\n",
    "try:\n",
    "    # Cr√©er un agent RAG avec l'outil recommand√© par la doc Red Hat\n",
    "    print(\"üîß Cr√©ation de l'agent RAG...\")\n",
    "    \n",
    "    rag_agent = Agent(\n",
    "        client,\n",
    "        model=\"llama-32-1b-instruct\",\n",
    "        instructions=\"Tu es un assistant IA utile et amical sp√©cialis√© dans l'assurance.\",\n",
    "        tools=[\n",
    "            {\n",
    "                \"name\": \"builtin::rag/knowledge_search\",\n",
    "                \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
    "            }\n",
    "        ],\n",
    "        config={\n",
    "            \"max_tokens\": 100,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Agent RAG cr√©√© avec l'outil knowledge_search\")\n",
    "    \n",
    "    # Cr√©er une session\n",
    "    session_id = rag_agent.create_session(session_name=f\"rag_test_{uuid.uuid4().hex[:8]}\")\n",
    "    print(f\"üìù Session cr√©√©e: {session_id}\")\n",
    "    \n",
    "    # Test avec une question sur l'assurance\n",
    "    prompt = \"Quelles sont les conditions g√©n√©rales d'assurance auto d'AXA ?\"\n",
    "    print(f\"‚ùì Question: {prompt}\")\n",
    "    \n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"üì§ R√©ponse de l'agent RAG:\")\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la cr√©ation de l'agent RAG: {e}\")\n",
    "    print(f\"Type d'erreur: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜï Test 7: Requ√™te vectorielle directe\n",
    "\n",
    "Test de la requ√™te vectorielle directe avec `client.vector_io.query()` selon la documentation Red Hat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: Requ√™te vectorielle directe\n",
    "print(\"üîç Test 7: Requ√™te vectorielle directe\")\n",
    "\n",
    "try:\n",
    "    # Test de requ√™te vectorielle directe selon la doc Red Hat\n",
    "    print(f\"üîç Requ√™te dans la base vectorielle: {vector_db_id}\")\n",
    "    \n",
    "    query_result = client.vector_io.query(\n",
    "        vector_db_id=vector_db_id,\n",
    "        query=\"assurance auto AXA conditions g√©n√©rales\",\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ R√©sultat de la requ√™te vectorielle:\")\n",
    "    print(f\"Type: {type(query_result)}\")\n",
    "    print(f\"Contenu: {query_result}\")\n",
    "    \n",
    "    # Si c'est une liste, afficher les √©l√©ments\n",
    "    if isinstance(query_result, list):\n",
    "        print(f\"üìÑ Nombre de documents trouv√©s: {len(query_result)}\")\n",
    "        for i, doc in enumerate(query_result[:3]):  # Afficher les 3 premiers\n",
    "            print(f\"  Document {i+1}: {doc}\")\n",
    "    elif isinstance(query_result, dict):\n",
    "        print(f\"üìÑ Cl√©s disponibles: {list(query_result.keys())}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de la requ√™te vectorielle: {e}\")\n",
    "    print(f\"Type d'erreur: {type(e).__name__}\")\n",
    "    \n",
    "    # Test alternatif avec une requ√™te plus simple\n",
    "    try:\n",
    "        print(\"\\nüîÑ Test alternatif avec requ√™te simple...\")\n",
    "        query_result_simple = client.vector_io.query(\n",
    "            vector_db_id=vector_db_id,\n",
    "            query=\"assurance\",\n",
    "        )\n",
    "        print(f\"‚úÖ R√©sultat simple: {query_result_simple}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Erreur requ√™te simple: {e2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
