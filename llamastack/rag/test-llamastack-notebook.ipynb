{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§ª Test LlamaStack dans OpenShift AI\n",
    "\n",
    "Ce notebook teste la fonctionnalitÃ© LlamaStack avec le modÃ¨le Llama-3.2-1B-Instruct dÃ©ployÃ© sur OpenShift AI.\n",
    "\n",
    "## ğŸ“‹ PrÃ©requis\n",
    "- Namespace: `llama-instruct-32-1b-demo`\n",
    "- LlamaStackDistribution: `lsd-llama-32-1b-instruct`\n",
    "- InferenceService: `llama-32-1b-instruct`\n",
    "\n",
    "## ğŸ”— Services utilisÃ©s\n",
    "- `lsd-llama-32-1b-instruct-service:8321` (LlamaStack)\n",
    "- `llama-32-1b-instruct-predictor:80` (vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dÃ©pendances\n",
    "!pip install llama-stack-client fire requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies\n",
    "import requests\n",
    "import json\n",
    "from llama_stack_client import Client, Agent, AgentEventLogger\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "print(\"âœ… Librairies importÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Test 1: VÃ©rification des services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de connexion Ã  LlamaStack\n",
    "llamastack_url = \"http://lsd-llama-32-1b-instruct-service:8321\"\n",
    "\n",
    "print(f\"ğŸ”— Test de connexion Ã  LlamaStack: {llamastack_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{llamastack_url}/v1/models\", timeout=10)\n",
    "    print(f\"âœ… Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"ğŸ“‹ ModÃ¨les disponibles:\")\n",
    "        for model in models.get('data', []):\n",
    "            print(f\"  - {model.get('identifier', 'N/A')} ({model.get('model_type', 'N/A')})\")\n",
    "    else:\n",
    "        print(f\"âŒ Erreur: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur de connexion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de connexion Ã  vLLM\n",
    "vllm_url = \"http://llama-32-1b-instruct-predictor:80\"\n",
    "\n",
    "print(f\"ğŸ”— Test de connexion Ã  vLLM: {vllm_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{vllm_url}/v1/models\", timeout=10)\n",
    "    print(f\"âœ… Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"ğŸ“‹ ModÃ¨les vLLM disponibles:\")\n",
    "        for model in models.get('data', []):\n",
    "            print(f\"  - {model.get('id', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"âŒ Erreur: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur de connexion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Test 2: Client LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion au client LlamaStack\n",
    "try:\n",
    "    client = Client(base_url=llamastack_url)\n",
    "    print(\"âœ… Client LlamaStack connectÃ©\")\n",
    "    \n",
    "    # Lister les modÃ¨les\n",
    "    models = client.models.list()\n",
    "    print(f\"ğŸ“‹ {len(models)} modÃ¨les trouvÃ©s:\")\n",
    "    for model in models:\n",
    "        print(f\"  - {model.identifier} ({model.model_type})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur client LlamaStack: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VÃ©rifier les bases vectorielles\n",
    "try:\n",
    "    vector_dbs = client.vector_dbs.list()\n",
    "    print(f\"ğŸ—„ï¸ {len(vector_dbs)} bases vectorielles trouvÃ©es:\")\n",
    "    for db in vector_dbs:\n",
    "        print(f\"  - {db.identifier}\")\n",
    "        \n",
    "    if not vector_dbs:\n",
    "        print(\"â„¹ï¸ Aucune base vectorielle trouvÃ©e (normal si pas encore d'ingestion)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur bases vectorielles: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Test 3: Agent LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un agent LlamaStack\n",
    "try:\n",
    "    # Trouver le modÃ¨le LLM\n",
    "    llm_model = None\n",
    "    for model in models:\n",
    "        if model.model_type == \"llm\":\n",
    "            llm_model = model\n",
    "            break\n",
    "    \n",
    "    if llm_model:\n",
    "        print(f\"ğŸ¤– ModÃ¨le LLM trouvÃ©: {llm_model.identifier}\")\n",
    "        \n",
    "        # CrÃ©er un agent simple (sans RAG pour l'instant)\n",
    "        agent = Agent(\n",
    "            client,\n",
    "            model=llm_model.identifier,\n",
    "            instructions=\"Tu es un assistant IA utile et amical.\"\n",
    "        )\n",
    "        print(\"âœ… Agent LlamaStack crÃ©Ã©\")\n",
    "        \n",
    "        # Test simple\n",
    "        session_id = agent.create_session(session_name=f\"test_session_{uuid.uuid4().hex[:8]}\")\n",
    "        print(f\"ğŸ“ Session crÃ©Ã©e: {session_id}\")\n",
    "        \n",
    "        # Question simple\n",
    "        response = agent.create_turn(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Bonjour, comment allez-vous ?\"}],\n",
    "            session_id=session_id,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        print(\"ğŸ’¬ RÃ©ponse:\")\n",
    "        for log in AgentEventLogger().log(response):\n",
    "            if hasattr(log, 'content') and log.content:\n",
    "                print(log.content, end='', flush=True)\n",
    "        print()\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ Aucun modÃ¨le LLM trouvÃ©\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Test 4: Test direct vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test direct de l'API vLLM\n",
    "print(\"ğŸ”§ Test direct de l'API vLLM\")\n",
    "\n",
    "try:\n",
    "    # Test de chat completion\n",
    "    chat_payload = {\n",
    "        \"model\": \"llama-32-1b-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explique-moi ce qu'est l'intelligence artificielle en 2 phrases.\"}\n",
    "        ],\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{vllm_url}/v1/chat/completions\",\n",
    "        json=chat_payload,\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"âœ… RÃ©ponse vLLM:\")\n",
    "        print(result['choices'][0]['message']['content'])\n",
    "    else:\n",
    "        print(f\"âŒ Erreur vLLM: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur test vLLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š RÃ©sumÃ© des tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰ Tests terminÃ©s !\")\n",
    "print(\"\\nğŸ“‹ Prochaines Ã©tapes:\")\n",
    "print(\"1. Si les tests passent, dÃ©ployer la pipeline d'ingestion RAG\")\n",
    "print(\"2. Tester la fonctionnalitÃ© RAG avec des documents\")\n",
    "print(\"3. Utiliser le use case assurance\")\n",
    "print(\"\\nğŸ”— Services testÃ©s:\")\n",
    "print(f\"  - LlamaStack: {llamastack_url}\")\n",
    "print(f\"  - vLLM: {vllm_url}\")\n",
    "print(\"\\nğŸ“š Documentation:\")\n",
    "print(\"  - README RAG: llamastack/rag/README.md\")\n",
    "print(\"  - Scripts de dÃ©ploiement: deploy-rag.sh, deploy-assurance.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• Test 5: Enregistrement de la base vectorielle (selon doc Red Hat)\n",
    "\n",
    "Test de l'approche recommandÃ©e par la documentation Red Hat pour enregistrer une base vectorielle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Enregistrement de la base vectorielle selon la doc Red Hat\n",
    "print(\"ğŸ”§ Test 5: Enregistrement de la base vectorielle\")\n",
    "\n",
    "# Configuration selon la documentation Red Hat\n",
    "vector_db_id = \"assurance_minio_db\"\n",
    "embedding_model_id = \"granite-embedding-125m\"\n",
    "embedding_dimension = 125\n",
    "provider_id = \"milvus\"\n",
    "\n",
    "try:\n",
    "    # Enregistrer la base vectorielle avec l'approche recommandÃ©e\n",
    "    print(f\"ğŸ“ Enregistrement de la base vectorielle: {vector_db_id}\")\n",
    "    \n",
    "    result = client.vector_dbs.register(\n",
    "        vector_db_id=vector_db_id,\n",
    "        embedding_model=embedding_model_id,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        provider_id=provider_id,\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Base vectorielle enregistrÃ©e: {result}\")\n",
    "    \n",
    "    # VÃ©rifier que la base est bien enregistrÃ©e\n",
    "    vector_dbs = client.vector_dbs.list()\n",
    "    print(f\"ğŸ—„ï¸ Bases vectorielles disponibles: {len(vector_dbs)}\")\n",
    "    for db in vector_dbs:\n",
    "        print(f\"  - {db.get('id', 'N/A')}: {db.get('name', 'N/A')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lors de l'enregistrement: {e}\")\n",
    "    print(f\"Type d'erreur: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• Test 6: Agent RAG avec outil knowledge_search\n",
    "\n",
    "Test de l'agent RAG avec l'outil `builtin::rag/knowledge_search` selon la documentation Red Hat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Agent RAG avec outil knowledge_search\n",
    "print(\"ğŸ¤– Test 6: Agent RAG avec outil knowledge_search\")\n",
    "\n",
    "try:\n",
    "    # CrÃ©er un agent RAG avec l'outil recommandÃ© par la doc Red Hat\n",
    "    print(\"ğŸ”§ CrÃ©ation de l'agent RAG...\")\n",
    "    \n",
    "    rag_agent = Agent(\n",
    "        client,\n",
    "        model=\"llama-32-1b-instruct\",\n",
    "        instructions=\"Tu es un assistant IA utile et amical spÃ©cialisÃ© dans l'assurance.\",\n",
    "        tools=[\n",
    "            {\n",
    "                \"name\": \"builtin::rag/knowledge_search\",\n",
    "                \"args\": {\"vector_db_ids\": [vector_db_id]},\n",
    "            }\n",
    "        ],\n",
    "        config={\n",
    "            \"max_tokens\": 100,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Agent RAG crÃ©Ã© avec l'outil knowledge_search\")\n",
    "    \n",
    "    # CrÃ©er une session\n",
    "    session_id = rag_agent.create_session(session_name=f\"rag_test_{uuid.uuid4().hex[:8]}\")\n",
    "    print(f\"ğŸ“ Session crÃ©Ã©e: {session_id}\")\n",
    "    \n",
    "    # Test avec une question sur l'assurance\n",
    "    prompt = \"Quelles sont les conditions gÃ©nÃ©rales d'assurance auto d'AXA ?\"\n",
    "    print(f\"â“ Question: {prompt}\")\n",
    "    \n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“¤ RÃ©ponse de l'agent RAG:\")\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lors de la crÃ©ation de l'agent RAG: {e}\")\n",
    "    print(f\"Type d'erreur: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ†• Test 7: RequÃªte vectorielle directe\n",
    "\n",
    "Test de la requÃªte vectorielle directe avec `client.vector_io.query()` selon la documentation Red Hat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 7: RequÃªte vectorielle directe\n",
    "print(\"ğŸ” Test 7: RequÃªte vectorielle directe\")\n",
    "\n",
    "try:\n",
    "    # Test de requÃªte vectorielle directe selon la doc Red Hat\n",
    "    print(f\"ğŸ” RequÃªte dans la base vectorielle: {vector_db_id}\")\n",
    "    \n",
    "    query_result = client.vector_io.query(\n",
    "        vector_db_id=vector_db_id,\n",
    "        query=\"assurance auto AXA conditions gÃ©nÃ©rales\",\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… RÃ©sultat de la requÃªte vectorielle:\")\n",
    "    print(f\"Type: {type(query_result)}\")\n",
    "    print(f\"Contenu: {query_result}\")\n",
    "    \n",
    "    # Si c'est une liste, afficher les Ã©lÃ©ments\n",
    "    if isinstance(query_result, list):\n",
    "        print(f\"ğŸ“„ Nombre de documents trouvÃ©s: {len(query_result)}\")\n",
    "        for i, doc in enumerate(query_result[:3]):  # Afficher les 3 premiers\n",
    "            print(f\"  Document {i+1}: {doc}\")\n",
    "    elif isinstance(query_result, dict):\n",
    "        print(f\"ğŸ“„ ClÃ©s disponibles: {list(query_result.keys())}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur lors de la requÃªte vectorielle: {e}\")\n",
    "    print(f\"Type d'erreur: {type(e).__name__}\")\n",
    "    \n",
    "    # Test alternatif avec une requÃªte plus simple\n",
    "    try:\n",
    "        print(\"\\nğŸ”„ Test alternatif avec requÃªte simple...\")\n",
    "        query_result_simple = client.vector_io.query(\n",
    "            vector_db_id=vector_db_id,\n",
    "            query=\"assurance\",\n",
    "        )\n",
    "        print(f\"âœ… RÃ©sultat simple: {query_result_simple}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Erreur requÃªte simple: {e2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
