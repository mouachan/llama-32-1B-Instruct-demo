{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Test LlamaStack dans OpenShift AI\n",
    "\n",
    "Ce notebook teste la fonctionnalit√© LlamaStack avec le mod√®le Llama-3.2-1B-Instruct d√©ploy√© sur OpenShift AI.\n",
    "\n",
    "## üìã Pr√©requis\n",
    "- Namespace: `llama-instruct-32-1b-demo`\n",
    "- LlamaStackDistribution: `lsd-llama-32-1b-instruct`\n",
    "- InferenceService: `llama-32-1b-instruct`\n",
    "\n",
    "## üîó Services utilis√©s\n",
    "- `lsd-llama-32-1b-instruct-service:8321` (LlamaStack)\n",
    "- `llama-32-1b-instruct-predictor:80` (vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install llama-stack-client fire requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies\n",
    "import requests\n",
    "import json\n",
    "from llama_stack_client import Client, Agent, AgentEventLogger\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Librairies import√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Test 1: V√©rification des services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de connexion √† LlamaStack\n",
    "llamastack_url = \"http://lsd-llama-32-1b-instruct-service:8321\"\n",
    "\n",
    "print(f\"üîó Test de connexion √† LlamaStack: {llamastack_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{llamastack_url}/v1/models\", timeout=10)\n",
    "    print(f\"‚úÖ Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"üìã Mod√®les disponibles:\")\n",
    "        for model in models.get('data', []):\n",
    "            print(f\"  - {model.get('identifier', 'N/A')} ({model.get('model_type', 'N/A')})\")\n",
    "    else:\n",
    "        print(f\"‚ùå Erreur: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de connexion √† vLLM\n",
    "vllm_url = \"http://llama-32-1b-instruct-predictor:80\"\n",
    "\n",
    "print(f\"üîó Test de connexion √† vLLM: {vllm_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{vllm_url}/v1/models\", timeout=10)\n",
    "    print(f\"‚úÖ Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        models = response.json()\n",
    "        print(\"üìã Mod√®les vLLM disponibles:\")\n",
    "        for model in models.get('data', []):\n",
    "            print(f\"  - {model.get('id', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Erreur: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de connexion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Test 2: Client LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion au client LlamaStack\n",
    "try:\n",
    "    client = Client(base_url=llamastack_url)\n",
    "    print(\"‚úÖ Client LlamaStack connect√©\")\n",
    "    \n",
    "    # Lister les mod√®les\n",
    "    models = client.models.list()\n",
    "    print(f\"üìã {len(models)} mod√®les trouv√©s:\")\n",
    "    for model in models:\n",
    "        print(f\"  - {model.identifier} ({model.model_type})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur client LlamaStack: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier les bases vectorielles\n",
    "try:\n",
    "    vector_dbs = client.vector_dbs.list()\n",
    "    print(f\"üóÑÔ∏è {len(vector_dbs)} bases vectorielles trouv√©es:\")\n",
    "    for db in vector_dbs:\n",
    "        print(f\"  - {db.identifier}\")\n",
    "        \n",
    "    if not vector_dbs:\n",
    "        print(\"‚ÑπÔ∏è Aucune base vectorielle trouv√©e (normal si pas encore d'ingestion)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur bases vectorielles: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ Test 3: Agent LlamaStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un agent LlamaStack\n",
    "try:\n",
    "    # Trouver le mod√®le LLM\n",
    "    llm_model = None\n",
    "    for model in models:\n",
    "        if model.model_type == \"llm\":\n",
    "            llm_model = model\n",
    "            break\n",
    "    \n",
    "    if llm_model:\n",
    "        print(f\"ü§ñ Mod√®le LLM trouv√©: {llm_model.identifier}\")\n",
    "        \n",
    "        # Cr√©er un agent simple (sans RAG pour l'instant)\n",
    "        agent = Agent(\n",
    "            client,\n",
    "            model=llm_model.identifier,\n",
    "            instructions=\"Tu es un assistant IA utile et amical.\"\n",
    "        )\n",
    "        print(\"‚úÖ Agent LlamaStack cr√©√©\")\n",
    "        \n",
    "        # Test simple\n",
    "        session_id = agent.create_session(session_name=f\"test_session_{uuid.uuid4().hex[:8]}\")\n",
    "        print(f\"üìù Session cr√©√©e: {session_id}\")\n",
    "        \n",
    "        # Question simple\n",
    "        response = agent.create_turn(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Bonjour, comment allez-vous ?\"}],\n",
    "            session_id=session_id,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        print(\"üí¨ R√©ponse:\")\n",
    "        for log in AgentEventLogger().log(response):\n",
    "            if hasattr(log, 'content') and log.content:\n",
    "                print(log.content, end='', flush=True)\n",
    "        print()\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Aucun mod√®le LLM trouv√©\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Test 4: Test direct vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test direct de l'API vLLM\n",
    "print(\"üîß Test direct de l'API vLLM\")\n",
    "\n",
    "try:\n",
    "    # Test de chat completion\n",
    "    chat_payload = {\n",
    "        \"model\": \"llama-32-1b-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Explique-moi ce qu'est l'intelligence artificielle en 2 phrases.\"}\n",
    "        ],\n",
    "        \"max_tokens\": 150,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{vllm_url}/v1/chat/completions\",\n",
    "        json=chat_payload,\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"‚úÖ R√©ponse vLLM:\")\n",
    "        print(result['choices'][0]['message']['content'])\n",
    "    else:\n",
    "        print(f\"‚ùå Erreur vLLM: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur test vLLM: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä R√©sum√© des tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Tests termin√©s !\")\n",
    "print(\"\\nüìã Prochaines √©tapes:\")\n",
    "print(\"1. Si les tests passent, d√©ployer la pipeline d'ingestion RAG\")\n",
    "print(\"2. Tester la fonctionnalit√© RAG avec des documents\")\n",
    "print(\"3. Utiliser le use case assurance\")\n",
    "print(\"\\nüîó Services test√©s:\")\n",
    "print(f\"  - LlamaStack: {llamastack_url}\")\n",
    "print(f\"  - vLLM: {vllm_url}\")\n",
    "print(\"\\nüìö Documentation:\")\n",
    "print(\"  - README RAG: llamastack/rag/README.md\")\n",
    "print(\"  - Scripts de d√©ploiement: deploy-rag.sh, deploy-assurance.sh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
