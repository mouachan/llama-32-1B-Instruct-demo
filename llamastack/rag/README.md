# ğŸ—„ï¸ RAG (Retrieval-Augmented Generation) avec LlamaStack

Ce rÃ©pertoire contient la configuration pour dÃ©ployer une fonctionnalitÃ© RAG (Retrieval-Augmented Generation) avec LlamaStack et Docling sur OpenShift AI.

## ğŸ“‹ Vue d'ensemble

La fonctionnalitÃ© RAG permet d'ingÃ©rer des documents dans une base vectorielle (Milvus) et d'utiliser ces informations pour enrichir les rÃ©ponses du modÃ¨le Llama-3.2-1B-Instruct.

### ğŸ—ï¸ Architecture RAG

```
Documents PDF â†’ Docling Pipeline â†’ Milvus Vector DB â†’ LlamaStack â†’ Llama Model
```

## ğŸ“ Structure des fichiers

```
llamastack/rag/
â”œâ”€â”€ README.md                    # Cette documentation
â”œâ”€â”€ docling-pipeline.yaml        # Pipeline Tekton pour l'ingestion de documents
â”œâ”€â”€ deploy-rag.sh               # Script de dÃ©ploiement automatisÃ©
â””â”€â”€ test-rag.py                 # Script de test de la fonctionnalitÃ© RAG
```

## ğŸš€ DÃ©ploiement

### PrÃ©requis

1. **LlamaStackDistribution** doit Ãªtre dÃ©ployÃ© et fonctionnel
2. **Namespace** `llama-instruct-32-1b-demo` doit exister
3. **Connexion OpenShift** active
4. **Python** avec `llama_stack_client` installÃ©

### DÃ©ploiement automatisÃ©

```bash
cd llamastack/rag
chmod +x deploy-rag.sh
./deploy-rag.sh
```

### DÃ©ploiement manuel

1. **DÃ©ployer le pipeline Docling :**
   ```bash
   oc apply -f docling-pipeline.yaml
   ```

2. **CrÃ©er un PipelineRun de test :**
   ```bash
   # Modifier les paramÃ¨tres selon vos besoins
   oc apply -f - << EOF
   apiVersion: tekton.dev/v1
   kind: PipelineRun
   metadata:
     name: docling-test-run
     namespace: llama-instruct-32-1b-demo
   spec:
     pipelineRef:
       name: docling-pipeline
     params:
       - name: base_url
         value: "https://example.com/pdfs"
       - name: pdf_filenames
         value: "document1.pdf,document2.pdf"
       - name: vector_db_id
         value: "my_milvus_db"
       - name: embed_model_id
         value: "granite-embedding-125m"
       - name: max_tokens
         value: "512"
       - name: use_gpu
         value: "false"
     workspaces:
       - name: shared-workspace
         volumeClaimTemplate:
           spec:
             accessModes: ["ReadWriteOnce"]
             resources:
               requests:
                 storage: 1Gi
   EOF
   ```

## ğŸ§ª Tests

### Test de la fonctionnalitÃ© RAG

```bash
# Configurer les variables d'environnement
export LLAMA_STACK_URL="http://localhost:8321"
export MODEL_ID="llama-32-1b-instruct"
export VECTOR_DB_ID="my_milvus_db"

# ExÃ©cuter le test
python3 test-rag.py
```

### VÃ©rification du statut

```bash
# VÃ©rifier le statut du PipelineRun
oc get pipelinerun -n llama-instruct-32-1b-demo

# Voir les logs du pipeline
oc logs pipelinerun/docling-test-run -n llama-instruct-32-1b-demo

# VÃ©rifier LlamaStackDistribution
oc get llamastackdistribution -n llama-instruct-32-1b-demo
```

## âš™ï¸ Configuration

### ParamÃ¨tres du pipeline Docling

| ParamÃ¨tre | Description | DÃ©faut |
|-----------|-------------|---------|
| `base_url` | URL de base pour rÃ©cupÃ©rer les PDFs | `https://example.com/pdfs` |
| `pdf_filenames` | Liste des fichiers PDF (sÃ©parÃ©s par virgules) | `document1.pdf,document2.pdf` |
| `num_workers` | Nombre de workers parallÃ¨les | `2` |
| `vector_db_id` | ID de la base vectorielle Milvus | `my_milvus_db` |
| `service_url` | URL du service Milvus | `http://milvus-standalone:19530` |
| `embed_model_id` | ID du modÃ¨le d'embedding | `granite-embedding-125m` |
| `max_tokens` | Nombre maximum de tokens par chunk | `512` |
| `use_gpu` | Activer l'accÃ©lÃ©ration GPU | `false` |

### ModÃ¨les d'embedding supportÃ©s

- `granite-embedding-125m` (IBM Granite)
- Autres modÃ¨les SentenceTransformers

## ğŸ” Utilisation

### RequÃªte RAG avec LlamaStack

```python
from llama_stack_client import Client, Agent, AgentEventLogger
import uuid

# Connexion au client
client = Client(base_url="http://localhost:8321")

# CrÃ©er un agent RAG
rag_agent = Agent(
    client,
    model="llama-32-1b-instruct",
    instructions="Tu es un assistant spÃ©cialisÃ© dans l'analyse de documents.",
    tools=[
        {
            "name": "builtin::rag/knowledge_search",
            "args": {"vector_db_ids": ["my_milvus_db"]},
        }
    ],
)

# CrÃ©er une session et poser une question
session_id = rag_agent.create_session(session_name=f"session_{uuid.uuid4().hex[:8]}")
response = rag_agent.create_turn(
    messages=[{"role": "user", "content": "Que sais-tu sur les documents analysÃ©s ?"}],
    session_id=session_id,
    stream=True,
)

# Afficher la rÃ©ponse
for log in AgentEventLogger().log(response):
    if hasattr(log, 'content') and log.content:
        print(log.content, end='', flush=True)
```

### RequÃªte directe sur la base vectorielle

```python
# Interroger directement la base vectorielle
query_result = client.vector_io.query(
    vector_db_id="my_milvus_db",
    query="informations importantes",
)

print(f"RÃ©sultats trouvÃ©s: {len(query_result.results)}")
for result in query_result.results:
    print(f"- {result.content[:100]}...")
```

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨mes courants

1. **PipelineRun Ã©choue :**
   - VÃ©rifier que l'URL des documents est accessible
   - VÃ©rifier les logs du pipeline
   - S'assurer que Milvus est accessible

2. **Test RAG Ã©choue :**
   - VÃ©rifier que LlamaStackDistribution est en phase "Ready"
   - VÃ©rifier l'URL de LlamaStack
   - S'assurer que la base vectorielle contient des donnÃ©es

3. **Erreur de connexion :**
   - VÃ©rifier les routes OpenShift
   - VÃ©rifier les services et pods
   - ContrÃ´ler les logs des pods

### Commandes de diagnostic

```bash
# VÃ©rifier les pods
oc get pods -n llama-instruct-32-1b-demo

# VÃ©rifier les services
oc get svc -n llama-instruct-32-1b-demo

# VÃ©rifier les routes
oc get route -n llama-instruct-32-1b-demo

# Logs de LlamaStack
oc logs -l app=llama-stack -n llama-instruct-32-1b-demo

# Logs du pipeline
oc logs pipelinerun/docling-test-run -n llama-instruct-32-1b-demo
```

## ğŸ“š Documentation

- [Red Hat OpenShift AI RAG Documentation](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.22/html/working_with_rag/deploying-a-rag-stack-in-a-data-science-project_rag)
- [LlamaStack Documentation](https://llama-stack.io/)
- [Docling Documentation](https://github.com/opendatahub-io/docling)

## ğŸ”„ Mise Ã  jour

Pour mettre Ã  jour la configuration RAG :

1. Modifier les fichiers YAML selon vos besoins
2. Appliquer les changements : `oc apply -f .`
3. RedÃ©marrer les ressources si nÃ©cessaire
4. Tester avec `python3 test-rag.py`
