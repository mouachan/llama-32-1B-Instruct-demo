# ğŸ““ Notebook de Test LlamaStack

Ce notebook Jupyter permet de tester la fonctionnalitÃ© LlamaStack dans OpenShift AI.

## ğŸ¯ Objectif

Tester la connexion et les fonctionnalitÃ©s de base de LlamaStack avec le modÃ¨le Llama-3.2-1B-Instruct dÃ©ployÃ©.

## ğŸ“‹ PrÃ©requis

- OpenShift AI Workbench (Data Science) dÃ©marrÃ©
- ModÃ¨le Llama-3.2-1B-Instruct dÃ©ployÃ© et fonctionnel
- LlamaStackDistribution dÃ©ployÃ©e et en cours d'exÃ©cution

## ğŸš€ Utilisation

1. **Ouvrir le notebook** dans OpenShift AI Workbench
2. **ExÃ©cuter les cellules** dans l'ordre
3. **VÃ©rifier les rÃ©sultats** de chaque test

## ğŸ”— Services testÃ©s

- **LlamaStack** : `http://lsd-llama-32-1b-instruct-service:8321`
- **vLLM** : `http://llama-32-1b-instruct-predictor:80`

## ğŸ“Š Tests inclus

1. **VÃ©rification des services** - Test de connexion HTTP
2. **Client LlamaStack** - Connexion et listing des modÃ¨les
3. **Agent LlamaStack** - CrÃ©ation d'un agent et test de conversation
4. **Base vectorielle** - Test RAG (si disponible)

## âš ï¸ Notes importantes

- Le notebook utilise uniquement les **services internes** (pas de routes externes)
- Les tests sont conÃ§us pour s'exÃ©cuter **dans le cluster OpenShift**
- Si les tests Ã©chouent, vÃ©rifiez que tous les services sont dÃ©marrÃ©s

## ğŸ”§ DÃ©pannage

Si les tests Ã©chouent :

1. VÃ©rifiez que LlamaStackDistribution est en cours d'exÃ©cution :
   ```bash
   oc get llamastackdistribution -n llama-instruct-32-1b-demo
   ```

2. VÃ©rifiez que l'InferenceService est prÃªt :
   ```bash
   oc get inferenceservice -n llama-instruct-32-1b-demo
   ```

3. Consultez les logs si nÃ©cessaire :
   ```bash
   oc logs -f deployment/lsd-llama-32-1b-instruct -n llama-instruct-32-1b-demo
   ```
