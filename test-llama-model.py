#!/usr/bin/env python3
"""
Test script pour le mod√®le Llama-3.2-1B-Instruct d√©ploy√© sur OpenShift AI
"""

import requests
import json
import time

# Configuration du mod√®le d√©ploy√©
import os
from urllib.parse import urljoin

# Chargement des variables d'environnement
OPENSHIFT_PROJECT = os.getenv("OPENSHIFT_PROJECT", "llama-instruct-32-1b-demo")
OPENSHIFT_CLUSTER_DOMAIN = os.getenv("OPENSHIFT_CLUSTER_DOMAIN")

if not OPENSHIFT_CLUSTER_DOMAIN:
    print("‚ùå Variable OPENSHIFT_CLUSTER_DOMAIN non d√©finie dans .env")
    print("Configurez votre fichier .env avec le bon domaine de cluster")
    exit(1)

# Construction de l'URL du mod√®le
MODEL_URL = f"https://llama-32-1b-instruct-{OPENSHIFT_PROJECT}.apps.{OPENSHIFT_CLUSTER_DOMAIN}"
CHAT_ENDPOINT = f"{MODEL_URL}/v1/chat/completions"
COMPLETION_ENDPOINT = f"{MODEL_URL}/v1/completions"
MODELS_ENDPOINT = f"{MODEL_URL}/v1/models"

def test_models_endpoint():
    """Test de l'endpoint /v1/models pour v√©rifier que le mod√®le est disponible"""
    print("üîç Test de l'endpoint /v1/models...")
    try:
        response = requests.get(MODELS_ENDPOINT, timeout=30)
        print(f"‚úÖ Status: {response.status_code}")
        if response.status_code == 200:
            models = response.json()
            print(f"üìã Mod√®les disponibles: {json.dumps(models, indent=2)}")
            return True
        else:
            print(f"‚ùå Erreur: {response.text}")
            return False
    except Exception as e:
        print(f"‚ùå Exception: {e}")
        return False

def test_chat_completion():
    """Test de l'endpoint /v1/chat/completions"""
    print("\nüí¨ Test de l'endpoint /v1/chat/completions...")
    
    payload = {
        "model": "llama-32-1b-instruct",
        "messages": [
            {
                "role": "system",
                "content": "Tu es un assistant IA utile et amical. R√©ponds en fran√ßais de mani√®re claire et concise."
            },
            {
                "role": "user",
                "content": "Salut ! Peux-tu me dire en quoi consiste le mod√®le Llama-3.2-1B-Instruct ?"
            }
        ],
        "max_tokens": 200,
        "temperature": 0.7,
        "stream": False
    }
    
    try:
        print("üì§ Envoi de la requ√™te...")
        response = requests.post(
            CHAT_ENDPOINT,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=60
        )
        
        print(f"‚úÖ Status: {response.status_code}")
        if response.status_code == 200:
            result = response.json()
            print(f"ü§ñ R√©ponse du mod√®le:")
            print(f"   - Mod√®le utilis√©: {result.get('model', 'N/A')}")
            print(f"   - Tokens utilis√©s: {result.get('usage', {}).get('total_tokens', 'N/A')}")
            print(f"   - R√©ponse: {result['choices'][0]['message']['content']}")
            return True
        else:
            print(f"‚ùå Erreur: {response.text}")
            return False
            
    except Exception as e:
        print(f"‚ùå Exception: {e}")
        return False

def test_completion():
    """Test de l'endpoint /v1/completions (format legacy)"""
    print("\nüìù Test de l'endpoint /v1/completions...")
    
    payload = {
        "model": "llama-32-1b-instruct",
        "prompt": "Explique-moi ce qu'est l'intelligence artificielle en 2 phrases:",
        "max_tokens": 100,
        "temperature": 0.5,
        "stream": False
    }
    
    try:
        print("üì§ Envoi de la requ√™te...")
        response = requests.post(
            COMPLETION_ENDPOINT,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=60
        )
        
        print(f"‚úÖ Status: {response.status_code}")
        if response.status_code == 200:
            result = response.json()
            print(f"ü§ñ R√©ponse du mod√®le:")
            print(f"   - Mod√®le utilis√©: {result.get('model', 'N/A')}")
            print(f"   - Tokens utilis√©s: {result.get('usage', {}).get('total_tokens', 'N/A')}")
            print(f"   - R√©ponse: {result['choices'][0]['text']}")
            return True
        else:
            print(f"‚ùå Erreur: {response.text}")
            return False
            
    except Exception as e:
        print(f"‚ùå Exception: {e}")
        return False

def main():
    """Fonction principale de test"""
    print("üöÄ Test du mod√®le Llama-3.2-1B-Instruct sur OpenShift AI")
    print("=" * 60)
    
    # Test 1: V√©rification des mod√®les disponibles
    models_ok = test_models_endpoint()
    
    if not models_ok:
        print("\n‚ùå Le mod√®le n'est pas accessible. V√©rifiez le statut de l'InferenceService.")
        return
    
    # Attendre un peu que le mod√®le soit compl√®tement charg√©
    print("\n‚è≥ Attente de 10 secondes pour que le mod√®le soit pr√™t...")
    time.sleep(10)
    
    # Test 2: Test de chat completion
    chat_ok = test_chat_completion()
    
    # Test 3: Test de completion
    completion_ok = test_completion()
    
    # R√©sum√© des tests
    print("\n" + "=" * 60)
    print("üìä R√âSUM√â DES TESTS")
    print("=" * 60)
    print(f"‚úÖ Endpoint /v1/models: {'OK' if models_ok else '√âCHEC'}")
    print(f"‚úÖ Chat completion: {'OK' if chat_ok else '√âCHEC'}")
    print(f"‚úÖ Completion: {'OK' if completion_ok else '√âCHEC'}")
    
    if all([models_ok, chat_ok, completion_ok]):
        print("\nüéâ TOUS LES TESTS SONT PASS√âS ! Le mod√®le Llama fonctionne parfaitement !")
    else:
        print("\n‚ö†Ô∏è  Certains tests ont √©chou√©. V√©rifiez la configuration du mod√®le.")

if __name__ == "__main__":
    main()
