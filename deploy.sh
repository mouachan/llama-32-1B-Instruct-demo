#!/bin/bash

# Script de dÃ©ploiement pour le modÃ¨le Llama-3.2-1B-Instruct sur OpenShift AI
# âš ï¸  Assurez-vous d'avoir copiÃ© env-template vers .env et configurÃ© vos valeurs !

set -e

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}ðŸš€ DÃ©ploiement du modÃ¨le Llama-3.2-1B-Instruct sur OpenShift AI${NC}"
echo "=================================================="

# VÃ©rification du fichier .env
if [ ! -f .env ]; then
    echo -e "${YELLOW}âš ï¸  Fichier .env non trouvÃ©, utilisation des valeurs par dÃ©faut${NC}"
    # DÃ©finition des valeurs par dÃ©faut
    export OPENSHIFT_PROJECT="llama-instruct-32-1b-demo"
    export HUGGINGFACE_TOKEN="VOTRE_TOKEN_HF_ICI"
    export GPU_TYPE="NVIDIA-A10G-PRIVATE"
    export GPU_COUNT="1"
    export GPU_MEMORY_UTILIZATION="0.95"
    export MODEL_MAX_LENGTH="4096"
    export MODEL_TENSOR_PARALLEL_SIZE="1"
    export CPU_REQUEST="2"
    export CPU_LIMIT="4"
    export MEMORY_REQUEST="6Gi"
    export MEMORY_LIMIT="8Gi"
    export PVC_SIZE="10Gi"
    export STORAGE_CLASS="gp3-csi"
    export API_PORT="8080"
    export API_TIMEOUT="60"
    export HUGGINGFACE_MODEL="meta-llama/Llama-3.2-1B-Instruct"
    
    # Affichage des valeurs par dÃ©faut utilisÃ©es
    echo -e "${YELLOW}ðŸ“‹ Valeurs par dÃ©faut utilisÃ©es :${NC}"
    echo "   PVC_SIZE: $PVC_SIZE"
    echo "   STORAGE_CLASS: $STORAGE_CLASS"
    echo "   GPU_TYPE: $GPU_TYPE"
    echo "   GPU_COUNT: $GPU_COUNT"
else
    # Chargement des variables d'environnement
    echo -e "${YELLOW}ðŸ“‹ Chargement des variables d'environnement...${NC}"
    source .env
fi

# VÃ©rification que les variables critiques sont dÃ©finies
echo -e "${YELLOW}ðŸ” VÃ©rification des variables critiques...${NC}"
echo "   PVC_SIZE: ${PVC_SIZE:-NON_DÃ‰FINI}"
echo "   STORAGE_CLASS: ${STORAGE_CLASS:-NON_DÃ‰FINI}"
echo "   GPU_TYPE: ${GPU_TYPE:-NON_DÃ‰FINI}"
echo "   GPU_COUNT: ${GPU_COUNT:-NON_DÃ‰FINI}"

# VÃ©rification des variables obligatoires
if [ "$HUGGINGFACE_TOKEN" = "VOTRE_TOKEN_HF_ICI" ]; then
    echo -e "${RED}âŒ Token Hugging Face non configurÃ© !${NC}"
    echo "Configurez HUGGINGFACE_TOKEN dans votre fichier .env ou modifiez le script"
    exit 1
fi

if [ -z "$GPU_TYPE" ] || [ "$GPU_TYPE" = "NVIDIA-A10G-PRIVATE" ]; then
    echo -e "${YELLOW}âš ï¸  Type de GPU non configurÃ©, utilisation de la valeur par dÃ©faut${NC}"
    echo "Configurez GPU_TYPE dans votre fichier .env pour votre cluster"
fi

# Encodage du token en base64
echo -e "${YELLOW}ðŸ” Encodage du token Hugging Face...${NC}"
HUGGINGFACE_TOKEN_B64=$(echo -n "$HUGGINGFACE_TOKEN" | base64)
export HUGGINGFACE_TOKEN_B64

# Encodage de l'URI PVC en base64
echo -e "${YELLOW}ðŸ”— Encodage de l'URI PVC...${NC}"
PVC_URI="pvc://pvc-llama32-1b-instruct"
PVC_URI_B64=$(echo -n "$PVC_URI" | base64)
export PVC_URI_B64

# Encodage des variables LlamaStack en base64
echo -e "${YELLOW}ðŸ” Encodage des variables LlamaStack...${NC}"
LLAMASTACK_INFERENCE_MODEL_B64=$(echo -n "${LLAMASTACK_INFERENCE_MODEL:-llama-32-1b-instruct}" | base64)
LLAMASTACK_VLLM_URL_B64=$(echo -n "${LLAMASTACK_VLLM_URL:-http://llama-32-1b-instruct-predictor:8080/v1}" | base64)
LLAMASTACK_VLLM_TLS_VERIFY_B64=$(echo -n "${LLAMASTACK_VLLM_TLS_VERIFY:-false}" | base64)
export LLAMASTACK_INFERENCE_MODEL_B64
export LLAMASTACK_VLLM_URL_B64
export LLAMASTACK_VLLM_TLS_VERIFY_B64

# VÃ©rification de la connexion OpenShift
echo -e "${YELLOW}ðŸ” VÃ©rification de la connexion OpenShift...${NC}"
if ! oc whoami >/dev/null 2>&1; then
    echo -e "${RED}âŒ Non connectÃ© Ã  OpenShift. Connectez-vous d'abord :${NC}"
    echo "oc login"
    exit 1
fi

echo -e "${GREEN}âœ… ConnectÃ© Ã  OpenShift en tant que : $(oc whoami)${NC}"

# CrÃ©ation du namespace
echo -e "${YELLOW}ðŸ“ CrÃ©ation du namespace...${NC}"
sed "s/\${OPENSHIFT_PROJECT}/$OPENSHIFT_PROJECT/g" namespace.yaml | oc apply -f -

# CrÃ©ation du secret Hugging Face
echo -e "${YELLOW}ðŸ” CrÃ©ation du secret Hugging Face...${NC}"
sed "s/\${HUGGINGFACE_TOKEN_B64}/$HUGGINGFACE_TOKEN_B64/g" secret-huggingface.yaml | oc apply -f -

# CrÃ©ation du PVC
echo -e "${YELLOW}ðŸ’¾ CrÃ©ation du PVC...${NC}"
echo -e "${YELLOW}ðŸ” Debug: Variables PVC_SIZE=$PVC_SIZE, STORAGE_CLASS=$STORAGE_CLASS${NC}"
echo -e "${YELLOW}ðŸ” Debug: Contenu du fichier PVC aprÃ¨s substitution:${NC}"
sed "s/\${PVC_SIZE}/$PVC_SIZE/g; s/\${STORAGE_CLASS}/$STORAGE_CLASS/g" pvc-llama32-1b-instruct.yaml
echo -e "${YELLOW}ðŸ” Application du PVC...${NC}"
sed "s/\${PVC_SIZE}/$PVC_SIZE/g; s/\${STORAGE_CLASS}/$STORAGE_CLASS/g" pvc-llama32-1b-instruct.yaml | oc apply -f -

# Le PVC sera bound quand il sera consommÃ© par le job
echo -e "${YELLOW}ðŸ’¾ PVC crÃ©Ã© (sera bound lors de l'utilisation)...${NC}"

# CrÃ©ation du secret de connexion PVC
echo -e "${YELLOW}ðŸ”— CrÃ©ation du secret de connexion PVC...${NC}"
HUGGINGFACE_MODEL_ESCAPED=$(echo "$HUGGINGFACE_MODEL" | sed 's/[\/&]/\\&/g')
sed "s/\${OPENSHIFT_PROJECT}/$OPENSHIFT_PROJECT/g; s/\${HUGGINGFACE_MODEL}/$HUGGINGFACE_MODEL_ESCAPED/g; s/\${PVC_URI_B64}/$PVC_URI_B64/g" secret-llama-model-pvc-connection.yaml | oc apply -f -

# CrÃ©ation du secret LlamaStack
echo -e "${YELLOW}ðŸ” CrÃ©ation du secret LlamaStack...${NC}"
sed "s/\${LLAMASTACK_INFERENCE_MODEL_B64}/$LLAMASTACK_INFERENCE_MODEL_B64/g; s/\${LLAMASTACK_VLLM_URL_B64}/$LLAMASTACK_VLLM_URL_B64/g; s/\${LLAMASTACK_VLLM_TLS_VERIFY_B64}/$LLAMASTACK_VLLM_TLS_VERIFY_B64/g" llamastack/llama-stack-inference-model-secret.yaml | oc apply -f -

# TÃ©lÃ©chargement du modÃ¨le
echo -e "${YELLOW}ðŸ“¥ TÃ©lÃ©chargement du modÃ¨le depuis Hugging Face...${NC}"
HUGGINGFACE_MODEL_ESCAPED=$(echo "$HUGGINGFACE_MODEL" | sed 's/[\/&]/\\&/g')
sed "s/\${HUGGINGFACE_MODEL}/$HUGGINGFACE_MODEL_ESCAPED/g" job-download-hf-cli.yaml | oc apply -f -

# Attente de la fin du tÃ©lÃ©chargement
echo -e "${YELLOW}â³ Attente de la fin du tÃ©lÃ©chargement...${NC}"
oc wait --for=condition=Complete job/download-llama32-1b-instruct-hf-cli -n llama-instruct-32-1b-demo --timeout=1800s

# VÃ©rification du contenu du PVC
echo -e "${YELLOW}ðŸ” VÃ©rification du contenu du PVC...${NC}"
echo "â³ Attente de 10 secondes pour que le PVC soit stable..."
sleep 10

# Nettoyage du pod de vÃ©rification existant s'il existe
echo -e "${YELLOW}ðŸ§¹ Nettoyage du pod de vÃ©rification existant...${NC}"
oc delete pod check-pvc-content -n llama-instruct-32-1b-demo --ignore-not-found=true
sleep 5

# CrÃ©ation d'un pod temporaire pour vÃ©rifier le contenu
echo -e "${YELLOW}ðŸ“ VÃ©rification du contenu tÃ©lÃ©chargÃ©...${NC}"
oc apply -f pod-check-pvc.yaml

# Attendre un peu que le pod dÃ©marre
echo -e "${YELLOW}â³ Attente du dÃ©marrage du pod (5s)...${NC}"
sleep 5

# VÃ©rifier le statut et essayer de rÃ©cupÃ©rer les logs
echo -e "${YELLOW}ðŸ” Statut du pod:${NC}"
oc get pod check-pvc-content -n llama-instruct-32-1b-demo

echo -e "${YELLOW}ðŸ“‹ Tentative de rÃ©cupÃ©ration des logs:${NC}"
if oc logs check-pvc-content -n llama-instruct-32-1b-demo --tail=20 2>/dev/null; then
    echo -e "${GREEN}âœ… Logs rÃ©cupÃ©rÃ©s avec succÃ¨s${NC}"
else
    echo -e "${YELLOW}âš ï¸  Pod pas encore prÃªt, continuation du dÃ©ploiement${NC}"
fi

# Nettoyage du pod de vÃ©rification
echo -e "${YELLOW}ðŸ§¹ Nettoyage du pod de vÃ©rification...${NC}"
oc delete pod check-pvc-content -n llama-instruct-32-1b-demo --ignore-not-found=true

# VÃ©rification que le modÃ¨le principal est prÃ©sent
echo -e "${YELLOW}âœ… VÃ©rification de la prÃ©sence du modÃ¨le...${NC}"
echo -e "${GREEN}âœ… ModÃ¨le dÃ©jÃ  vÃ©rifiÃ© dans le PVC, continuation du dÃ©ploiement${NC}"

# CrÃ©ation du ServingRuntime
echo -e "${YELLOW}âš™ï¸  CrÃ©ation du ServingRuntime...${NC}"
sed "s/\${API_PORT}/$API_PORT/g" servingruntime-llama32-1b.yaml | oc apply -f -

# CrÃ©ation de l'InferenceService
echo -e "${YELLOW}ðŸš€ CrÃ©ation de l'InferenceService...${NC}"
# Ã‰chapper les caractÃ¨res spÃ©ciaux pour sed
GPU_TYPE_ESCAPED=$(echo "$GPU_TYPE" | sed 's/[\/&]/\\&/g')
HUGGINGFACE_MODEL_ESCAPED=$(echo "$HUGGINGFACE_MODEL" | sed 's/[\/&]/\\&/g')

# CrÃ©er un fichier temporaire avec les substitutions
sed "s/\${GPU_TYPE}/$GPU_TYPE_ESCAPED/g; s/\${GPU_COUNT}/$GPU_COUNT/g; s/\${GPU_MEMORY_UTILIZATION}/$GPU_MEMORY_UTILIZATION/g; s/\${MODEL_MAX_LENGTH}/$MODEL_MAX_LENGTH/g; s/\${MODEL_TENSOR_PARALLEL_SIZE}/$MODEL_TENSOR_PARALLEL_SIZE/g; s/\${CPU_REQUEST}/$CPU_REQUEST/g; s/\${CPU_LIMIT}/$CPU_LIMIT/g; s/\${MEMORY_REQUEST}/$MEMORY_REQUEST/g; s/\${MEMORY_LIMIT}/$MEMORY_LIMIT/g; s/\${HUGGINGFACE_MODEL}/$HUGGINGFACE_MODEL_ESCAPED/g" inferenceservice-llama32-1b.yaml > /tmp/inferenceservice-temp.yaml

# Appliquer le fichier temporaire
oc apply -f /tmp/inferenceservice-temp.yaml

# Nettoyer
rm -f /tmp/inferenceservice-temp.yaml

# CrÃ©ation de la LlamaStackDistribution
echo -e "${YELLOW}ðŸ¤– CrÃ©ation de la LlamaStackDistribution...${NC}"
oc apply -f llamastack/llama-stack-distribution.yaml

echo -e "${GREEN}âœ… DÃ©ploiement terminÃ© !${NC}"
echo ""
echo -e "${BLUE}ðŸ“‹ Prochaines Ã©tapes :${NC}"
echo "1. Attendez que l'InferenceService soit prÃªt :"
echo "   oc get inferenceservice -n llama-instruct-32-1b-demo"
echo ""
echo "2. Testez le modÃ¨le :"
echo "   python3 test-llama-model.py"
echo ""
echo "3. AccÃ©dez Ã  OpenShift AI pour voir votre modÃ¨le dÃ©ployÃ©"
echo ""
echo -e "${GREEN}ðŸŽ‰ Bonne utilisation !${NC}"
